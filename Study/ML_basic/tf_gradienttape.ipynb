{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt \n",
    "\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow는 gradient 계산 시에 forward 과정에서 순차적으로 어떤 노드를 거쳤는지 기억해서 이를 역순으로 gradient descent를 거칩니다. 이때 사용하는 것이 gradient tape라고 합니다. \n",
    "tf.GradientTape는 context 안에서 실행되는 모든 연산을 tape에 record합니다. 그리고 기록된 연산의 그레디언트를 계산합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx = tape.gradient(y, x) #이때 GradientTape.gradient(target, sources)에서 target은 실제 y_hat , x는 입력값이라고 생각하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[0.03123844, 2.434194  ],\n",
       "        [0.06247687, 4.868388  ],\n",
       "        [0.09371531, 7.3025823 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.03123844, 2.434194  ], dtype=float32)>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.Variable(tf.random.normal((3,2)), name=\"w\") \n",
    "b = tf.Variable(tf.zeros(2,dtype=tf.float32), name='b')\n",
    "x= [[1. ,2. ,3.]]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape: # persistent = True로 진행한다면, 같은 computation을 여러 번 진행하기 위해서입니다.\n",
    "    y = tf.matmul(x,w) + b\n",
    "    loss = tf.reduce_mean(y**2) # \n",
    "grad = tape.gradient(loss, [w,b] )# 한 개보다 많은 변수에 대해 backward 과정을 거칩니다. \n",
    "# 이때 tape.gradient에 backward를 해야하는 변수들을 리스트 형태로 넣기에 \n",
    "# grad[0] 에는 w, grad[1]에는 b가 저장된다. \n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[0.03123844, 2.434194  ],\n",
       "       [0.06247687, 4.868388  ],\n",
       "       [0.09371531, 7.3025823 ]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_options = {\n",
    "    \"w\": w,\n",
    "    \"b\": b\n",
    "}\n",
    "grad_dictionary = tape.gradient(loss, grad_options)\n",
    "grad_dictionary['w'] #dictionary 형태로 tape.gradient에 넣으면 dictionary 형태로 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model에서 사용할 gradient \n",
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2. , 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "\n",
    "    y = layer(x) \n",
    "    loss = tf.reduce_mean(y**2) #cost function which is sum of error \n",
    "grad = tape.gradient(loss, layer.trainable_variables) # train할 수 있는 layer의 변수들을 넣어주는 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_2/kernel:0, shape: (3, 2)\n",
      "dense_2/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "  print(f'{var.name}, shape: {g.shape}') # Kernel은 layer에서 정해진 weight matrix\n",
    "                                         # kernel is a weights matrix created by the layer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# gradient에 unconnect일때 None이 나오는 대신 0을 가져오는 것으로 바꿀 수 있습니다.\n",
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y**2\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Tape 에서 gradient descent 할 변수와 하지 않을 변수를 따로 결정할 수 있습니다. \n",
    "x = tf.Variable([4., 2.], trainable=False) # gradient 진행을 안 합니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래와 같이 nested with priority도 가능합니다. \n",
    "x = tf.constant(5.0)\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  with tf.GradientTape() as gg:\n",
    "    gg.watch(x)\n",
    "    y = x * x\n",
    "  dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n",
    "d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 0 # 예시\n",
    "flag = 12 \n",
    "with tf.GradientTape() as tape:\n",
    "    los= example\n",
    "    if los > flag:\n",
    "        tape.reset() # reset은 tape에 정보를 초기화해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(4.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  with tape.stop_recording(): # 일시적으로 tape에 record를 중지하는 방식입니다.\n",
    "    y = x ** 2\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82f830dbf2bedede5f6a6fd84a718d579e894f86bdba96d28c0990c901c667f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
